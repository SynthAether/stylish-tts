# Configuration for training on a high-resource (24GB+ VRAM) GPU.

multispeaker: false
n_mels: 80
sample_rate: 24000
n_fft: 1024
win_length: 1024
hop_length: 300
style_dim: 64
inter_dim: 128

text_aligner:
  hidden_dim: 256
  token_embedding_dim: 512

decoder:
  hidden_dim: 256
  residual_dim: 64

# generator:
#   type: 'ringformer'
#   resblock_kernel_sizes: [ 3, 7, 11 ]
#   upsample_rates: [ 4, 5 ]
#   upsample_initial_channel: 512
#   upsample_last_channel: 128
#   resblock_dilation_sizes: [ [ 1, 3, 5 ], [ 1, 3, 5 ], [ 1, 3, 5 ] ]
#   upsample_kernel_sizes: [ 8, 10 ]
#   gen_istft_n_fft: 60
#   gen_istft_hop_size: 15
#   depth: 2

generator:
  type: 'freegan'
  input_dim: 256
  hidden_dim: 513
  conv_intermediate_dim: 1024
  io_conv_kernel_size: 7
  conformer_layers: 1
  conv_layers: 4

text_encoder:
  tokens: 178 # number of phoneme tokens
  hidden_dim: 128
  filter_channels: 512
  heads: 8
  layers: 8
  kernel_size: 3
  dropout: 0.2

style_encoder:
  layers: 2

mel_style_encoder:
  max_channels: 384
  skip_downsample: True

duration_predictor:
  n_layer: 3
  duration_classes: 16
  max_duration: 50
  dropout: 0.5
  last_dropout: 0.5

pitch_energy_predictor:
  inter_dim: 512
  dropout: 0.2

# speech language model config
slm:
  model: 'microsoft/wavlm-base-plus'
  sr: 16000 # sampling rate of SLM

symbol:
  pad: "$"
  punctuation: ";:,.!?¡¿—…\"()“” "
  letters: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
  letters_ipa: "ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁᵊǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ"
